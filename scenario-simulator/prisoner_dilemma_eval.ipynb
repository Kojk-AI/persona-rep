{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import glob, os\n",
    "from sfem import SFEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(df, df2):\n",
    "    \n",
    "    #score\n",
    "    score = 0\n",
    "    for row in range(0,df.shape[0]):\n",
    "        if df['A'].iloc[row] == \"cooperate\" and df['B'].iloc[row] == \"cooperate\":\n",
    "            score +=1\n",
    "        if df['A'].iloc[row] == \"cooperate\" and df['B'].iloc[row] == \"defect\":\n",
    "            score +=5\n",
    "        if df['A'].iloc[row] == \"defect\" and df['B'].iloc[row] == \"defect\":\n",
    "            score +=3\n",
    "\n",
    "    #total_score\n",
    "    total_score = 0\n",
    "    for row in range(0,df.shape[0]):\n",
    "        if df['A'].iloc[row] == \"cooperate\" and df['B'].iloc[row] == \"cooperate\":\n",
    "            total_score +=2\n",
    "        if df['A'].iloc[row] == \"cooperate\" and df['B'].iloc[row] == \"defect\":\n",
    "            total_score +=5\n",
    "        if df['A'].iloc[row] == \"defect\" and df['B'].iloc[row] == \"defect\":\n",
    "            total_score +=6\n",
    "        if df['A'].iloc[row] == \"defect\" and df['B'].iloc[row] == \"cooperate\":\n",
    "            total_score +=5\n",
    "\n",
    "    #defect rate\n",
    "    defect_rate = df['A'][df['A']==\"defect\"].count()/df['A'].count()\n",
    "\n",
    "    #Reciprocity rate\n",
    "    rr_count = 0\n",
    "    rr_invalid = 0\n",
    "    for row in range(1,df.shape[0]):\n",
    "        if df['A'].iloc[row] != \"None\" and df['B'].iloc[row-1] != \"None\":\n",
    "            if df['A'].iloc[row] == df['B'].iloc[row-1]:\n",
    "                rr_count +=1\n",
    "        else:\n",
    "            rr_invalid +=1\n",
    "    rr_percentage = rr_count/(df.shape[0]-1)\n",
    "\n",
    "    #Forgiveness rate based off paper \"Nicer Than Humans: How do Large Language Models Behave in the Prisonerâ€™s Dilemma?\"\n",
    "    f_count = 0\n",
    "    opp_betray_count = 0\n",
    "    penalty = 0\n",
    "    f_invalid = 0\n",
    "    vengeful = False\n",
    "    for row in range(1,df.shape[0]):\n",
    "        if df['A'].iloc[row] != \"None\" and df['A'].iloc[row-1] != \"None\" and df['B'].iloc[row-1] != \"None\":\n",
    "            if df['B'].iloc[row-1] == \"defect\":\n",
    "                opp_betray_count += 1\n",
    "                if df['A'].iloc[row] == \"cooperate\":\n",
    "                    f_count += 1\n",
    "                    vengeful = False\n",
    "            if row > 1:\n",
    "                if df['B'].iloc[row-2] != \"None\":\n",
    "                    if df['B'].iloc[row-2] == \"defect\" and df['B'].iloc[row-1] == \"cooperate\":\n",
    "                        if df['A'].iloc[row] == \"defect\" and df['A'].iloc[row-1] == \"defect\":\n",
    "                            penalty += 1\n",
    "                            vengeful = True\n",
    "                        else:\n",
    "                            vengeful = False\n",
    "                    else:\n",
    "                        if vengeful and df['A'].iloc[row] == \"defect\":\n",
    "                            penalty += 1\n",
    "                        else:\n",
    "                            vengeful = False\n",
    "                else:\n",
    "                    f_invalid +=1\n",
    "        else:\n",
    "            f_invalid +=1\n",
    "\n",
    "    if opp_betray_count == 0:\n",
    "        f_percentage = -1\n",
    "    else:\n",
    "        f_percentage = f_count / (opp_betray_count + penalty)\n",
    "\n",
    "    #Forgiveness rate based on  we only count forgiveness if the agent plays betrays after opponent betrays and then revert back to cooperation if the opponent seeks forgiveness (switches to cooperates). Penalty term remains.\n",
    "    # f_count = 0\n",
    "    # opp_betray_count = 0\n",
    "    # penalty = 0\n",
    "    # f_invalid = 0\n",
    "    # vengeful = False\n",
    "    # for row in range(1,df.shape[0]):\n",
    "    #     if df['invalid'].iloc[row] == False and df['invalid'].iloc[row-1] == False:\n",
    "    #         if df['betray'].iloc[row-1] == True:\n",
    "    #             opp_betray_count += 1\n",
    "    #         if row > 1:\n",
    "    #             if df['betray'].iloc[row-2] == True and df['betray'].iloc[row-1] == False:\n",
    "    #                 if df['betray'].iloc[row] == True and df['betray'].iloc[row-1] == True:\n",
    "    #                     penalty += 1\n",
    "    #                     vengeful = True\n",
    "    #                 elif df['betray'].iloc[row] == True and df['betray'].iloc[row-1] == False:\n",
    "    #                     f_count += 1\n",
    "    #                     vengeful = False\n",
    "    #         else:\n",
    "    #             if vengeful and df['betray'].iloc[row] == True:\n",
    "    #                 penalty += 1\n",
    "    #             else:\n",
    "    #                 vengeful = False\n",
    "    #     else:\n",
    "    #         f_invalid +=1\n",
    "\n",
    "    # f_percentage = f_count / (opp_betray_count + penalty)\n",
    "\n",
    "    #Exploitability\n",
    "    e_count = 0\n",
    "    opp_betray_count = 0\n",
    "    e_invalid = 0\n",
    "    for row in range(1,df.shape[0]):\n",
    "        if df['A'].iloc[row] != \"None\" and df['B'].iloc[row-1] != \"None\":\n",
    "            if df['B'].iloc[row-1] == \"defect\":\n",
    "                opp_betray_count += 1\n",
    "                if df['A'].iloc[row] == \"cooperate\":\n",
    "                    e_count += 1\n",
    "        else:\n",
    "            e_invalid +=1\n",
    "            \n",
    "    if opp_betray_count == 0:\n",
    "        e_percentage = -1\n",
    "    else:\n",
    "        e_percentage = e_count / opp_betray_count\n",
    "\n",
    "    #Retaliatory\n",
    "    r_count = 0\n",
    "    opp_betray_count = 0\n",
    "    r_invalid = 0\n",
    "    for row in range(1,df.shape[0]):\n",
    "        if df['A'].iloc[row] != \"None\" and df['A'].iloc[row-1] != \"None\" and df['B'].iloc[row-1] != \"None\":\n",
    "            if df['A'].iloc[row-1] == \"cooperate\" and df['B'].iloc[row-1] == \"defect\":\n",
    "                opp_betray_count += 1\n",
    "                if df['A'].iloc[row] == \"defect\":\n",
    "                    r_count += 1\n",
    "        else:\n",
    "            r_invalid +=1\n",
    "\n",
    "    if opp_betray_count == 0:\n",
    "        r_percentage = -1\n",
    "    else:\n",
    "        r_percentage = r_count / opp_betray_count\n",
    "\n",
    "    #Troublemaking\n",
    "    t_count = 0\n",
    "    opportunity_count = 0\n",
    "    t_invalid = 0\n",
    "    for row in range(0,df.shape[0]):\n",
    "        if row == 0:\n",
    "            if df['A'].iloc[row] != \"None\":\n",
    "                opportunity_count += 1\n",
    "                if df['A'].iloc[row] == \"defect\":\n",
    "                    t_count +=1\n",
    "            else:\n",
    "                t_invalid +=1 \n",
    "        else:        \n",
    "            if df['A'].iloc[row] != \"None\" and df['B'].iloc[row-1] != \"None\":\n",
    "                if df['B'].iloc[row-1] == \"cooperate\":\n",
    "                    opportunity_count += 1\n",
    "                    if df['A'].iloc[row] == \"defect\":\n",
    "                        t_count += 1\n",
    "            else:\n",
    "                t_invalid +=1\n",
    "                \n",
    "    if opportunity_count == 0:\n",
    "        t_percentage = -1\n",
    "    else:\n",
    "        t_percentage = t_count / opportunity_count\n",
    "\n",
    "    if df2 is not None:\n",
    "        #Lying\n",
    "        l_count = 0\n",
    "        opportunity_count = 0\n",
    "        l_invalid = 0\n",
    "        for row in range(0,df.shape[0]):\n",
    "            if df['A'].iloc[row] != df2['A'].iloc[row]:\n",
    "                opportunity_count += 1\n",
    "                l_count += 1\n",
    "            else:\n",
    "                opportunity_count += 1\n",
    "                    \n",
    "        if opportunity_count == 0:\n",
    "            l_percentage = -1\n",
    "        else:\n",
    "            l_percentage = l_count / opportunity_count\n",
    "        #betrayal\n",
    "        b_count = 0\n",
    "        opportunity_count = 0\n",
    "        b_invalid = 0\n",
    "        for row in range(0,df.shape[0]):\n",
    "            if df2['B'].iloc[row]==\"cooperate\":\n",
    "                if df['A'].iloc[row] == \"defect\" and df2['A'].iloc[row]==\"cooperate\":\n",
    "                    opportunity_count += 1\n",
    "                    b_count += 1\n",
    "                else:\n",
    "                    opportunity_count += 1\n",
    "                    \n",
    "        if opportunity_count == 0:\n",
    "            b_percentage = -1\n",
    "        else:\n",
    "            b_percentage = b_count / opportunity_count\n",
    "        #manipulative\n",
    "        m_count = 0\n",
    "        opportunity_count = 0\n",
    "        m_invalid = 0\n",
    "        for row in range(0,df.shape[0]):\n",
    "            if df2['B'].iloc[row]==\"defect\":\n",
    "                if df['A'].iloc[row] == \"defect\" and df2['A'].iloc[row]==\"cooperate\":\n",
    "                    opportunity_count += 1\n",
    "                    m_count += 1\n",
    "                else:\n",
    "                    opportunity_count += 1\n",
    "                    \n",
    "        if opportunity_count == 0:\n",
    "            m_percentage = -1\n",
    "        else:\n",
    "            m_percentage = m_count / opportunity_count\n",
    "        #trusting\n",
    "        tr_count = 0\n",
    "        opportunity_count = 0\n",
    "        tr_invalid = 0\n",
    "        for row in range(0,df.shape[0]):\n",
    "            if df2['B'].iloc[row]==\"cooperate\":\n",
    "                if df['A'].iloc[row] == \"cooperate\" and df2['A'].iloc[row]==\"cooperate\":\n",
    "                    opportunity_count += 1\n",
    "                    tr_count += 1\n",
    "                else:\n",
    "                    opportunity_count += 1\n",
    "                    \n",
    "        if opportunity_count == 0:\n",
    "            tr_percentage = -1\n",
    "        else:\n",
    "            tr_percentage = tr_count / opportunity_count\n",
    "    else:\n",
    "        l_percentage = -1\n",
    "        m_percentage = -1\n",
    "        b_percentage = -1\n",
    "        tr_percentage = -1\n",
    "        \n",
    "    #SFEM\n",
    "    sfem = SFEM(df)\n",
    "    sfem_res = sfem.save_results()\n",
    "    \n",
    "    # Return the result as a dictionary\n",
    "    results = {\n",
    "        'defect_rate': defect_rate,\n",
    "        'reciprocity_rate': rr_percentage,\n",
    "        'reciprocity_invalid': rr_invalid,\n",
    "        'forgiveness_rate': f_percentage,\n",
    "        'forgiveness_invalid': f_invalid,\n",
    "        'exploitability_rate': e_percentage,\n",
    "        'explotability_invalid': e_invalid,\n",
    "        'retaliatory_rate': r_percentage,\n",
    "        'rectaliatory_invalid': r_invalid,\n",
    "        'troublemaking_rate': t_percentage,\n",
    "        'troublemaking_invalid': t_invalid,    \n",
    "        'lying_rate': l_percentage,\n",
    "        'betrayal_rate': b_percentage,\n",
    "        'manipulative_rate': m_percentage,\n",
    "        'trusting_rate': tr_percentage,\n",
    "        'score': score,\n",
    "        'total_score': total_score,\n",
    "        'sfem': sfem_res    \n",
    "\t}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = ['s01_allc', 's02_alld', 's03_tft', 's04_dtft', 's05_tf2t', 's06_tf3t', 's07_2tft', 's08_2tf2t', 's09_t2', 's10_grim', 's11_grim2', 's12_grim3', 's13_wsls', 's14_2wsls', 's15_ctod', 's16_dtf2t', 's17_dtf3t', 's18_dgrim2', 's19_dgrim3', 's20_dcalt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in glob.glob(\"output/*llmvllm*\"):\n",
    "    print(exp)\n",
    "    results = []\n",
    "    for i in range(25):\n",
    "        with open(os.path.join(exp,\"prisoner_A_log_history_iteration_{}.json\".format(i)), \"r\") as f:\n",
    "            j = json.load(f)\n",
    "        df = pd.DataFrame(j)\n",
    "        df.columns = [\"A\", \"B\"]\n",
    "        with open(os.path.join(exp,\"prisoner_A_log_history_iteration_comms_{}.json\".format(i)), \"r\") as f:\n",
    "            j = json.load(f)\n",
    "        df2 = pd.DataFrame(j)\n",
    "        df2.columns = [\"A\", \"B\"]\n",
    "        res = metrics(df, df2)\n",
    "        for strat in strategies:\n",
    "            res[strat] = res['sfem'].loc[strat][\"Estimates\"]\n",
    "        results.append(res)\n",
    "    pd.DataFrame(results).to_csv(os.path.join(exp,\"results.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "opponents = [\"-AC*\"]\n",
    "\n",
    "for opp in opponents:\n",
    "\n",
    "    summary = pd.DataFrame()\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize=(10, 6))\n",
    "\n",
    "    for exp in glob.glob(f\"output/*baseline{opp}\"):\n",
    "        exp_res = os.path.join(exp, \"results.csv\")\n",
    "        df = pd.read_csv(exp_res)\n",
    "\n",
    "        df_metrics = pd.DataFrame()\n",
    "        metrics = [\"troublemaking_rate\"]\n",
    "        for m in metrics:\n",
    "            df_metrics[m] = df[m]\n",
    "\n",
    "        df_metrics.replace(-1, np.nan, inplace=True)\n",
    "        mask = ~np.isnan(df_metrics)\n",
    "        filtered_data = [df_metrics[d][mask[m]] for d, m in zip(df_metrics, mask)]\n",
    "        filtered_data_ = []\n",
    "        for f in filtered_data:\n",
    "            if not f.any():\n",
    "                f = [0]\n",
    "            filtered_data_.append(f)\n",
    "        \n",
    "        summary[exp]=filtered_data_[0]\n",
    "\n",
    "    exp_list = glob.glob(f\"output/*{opp}\")\n",
    "    exp_list.remove(exp)\n",
    "    exp_list.sort()\n",
    "\n",
    "    for i, exp in enumerate(exp_list):\n",
    "        exp_res = os.path.join(exp, \"results.csv\")\n",
    "        df = pd.read_csv(exp_res)\n",
    "\n",
    "        df_metrics = pd.DataFrame()\n",
    "        metrics = [\"troublemaking_rate\"]\n",
    "        for m in metrics:\n",
    "            df_metrics[m] = df[m]\n",
    "\n",
    "        df_metrics.replace(-1, np.nan, inplace=True)\n",
    "        mask = ~np.isnan(df_metrics)\n",
    "        filtered_data = [df_metrics[d][mask[m]] for d, m in zip(df_metrics, mask)]\n",
    "        filtered_data_ = []\n",
    "        for f in filtered_data:\n",
    "            if not f.any():\n",
    "                f = [0]\n",
    "            filtered_data_.append(f)\n",
    "\n",
    "        summary[exp]=filtered_data_[0]\n",
    "    # fig.savefig(f\"{opp}.png\")\n",
    "    x_labels = []\n",
    "    for col in summary.columns:\n",
    "        col = col.strip(\"output/\")\n",
    "        x_labels.append(col)\n",
    "\n",
    "    mask = ~np.isnan(summary)\n",
    "    filtered_data = [summary[d][mask[m]] for d, m in zip(summary, mask)]\n",
    "    filtered_data_ = []\n",
    "    for f in filtered_data:\n",
    "        if not f.any():\n",
    "            f = [0]\n",
    "        filtered_data_.append(f)\n",
    "    \n",
    "    ax.boxplot(filtered_data_)\n",
    "    ax.set_title(\"Troublemaking rate\")\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.set_xticks(range(1,22), labels=x_labels)\n",
    "    ax.tick_params(axis='x', labelrotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# metrics = [\"defect_rate\", \"reciprocity_rate\", \"forgiveness_rate\", \"exploitability_rate\", \"retaliatory_rate\", \"troublemaking_rate\"]\n",
    "opponents = [\"-RND-comms-selfish\"]\n",
    "\n",
    "for opp in opponents:\n",
    "\n",
    "    summary = pd.DataFrame()\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize=(10, 6))\n",
    "\n",
    "    for exp in glob.glob(f\"output/*baseline{opp}\"):\n",
    "        exp_res = os.path.join(exp, \"results.csv\")\n",
    "        df = pd.read_csv(exp_res)\n",
    "\n",
    "        df_metrics = pd.DataFrame()\n",
    "        metrics = [\"lying_rate\"]\n",
    "        for m in metrics:\n",
    "            df_metrics[m] = df[m]\n",
    "\n",
    "        df_metrics.replace(-1, np.nan, inplace=True)\n",
    "        mask = ~np.isnan(df_metrics)\n",
    "        filtered_data = [df_metrics[d][mask[m]] for d, m in zip(df_metrics, mask)]\n",
    "        filtered_data_ = []\n",
    "        for f in filtered_data:\n",
    "            if not f.any():\n",
    "                f = [0]\n",
    "            filtered_data_.append(f)\n",
    "        \n",
    "        summary[exp]=filtered_data_[0]\n",
    "\n",
    "    exp_list = glob.glob(f\"output/*{opp}\")\n",
    "    exp_list.remove(exp)\n",
    "    exp_list.sort()\n",
    "\n",
    "    for i, exp in enumerate(exp_list):\n",
    "        exp_res = os.path.join(exp, \"results.csv\")\n",
    "        df = pd.read_csv(exp_res)\n",
    "\n",
    "        df_metrics = pd.DataFrame()\n",
    "        metrics = [\"lying_rate\"]\n",
    "        for m in metrics:\n",
    "            df_metrics[m] = df[m]\n",
    "\n",
    "        df_metrics.replace(-1, np.nan, inplace=True)\n",
    "        mask = ~np.isnan(df_metrics)\n",
    "        filtered_data = [df_metrics[d][mask[m]] for d, m in zip(df_metrics, mask)]\n",
    "        filtered_data_ = []\n",
    "        for f in filtered_data:\n",
    "            if not f.any():\n",
    "                f = [0]\n",
    "            filtered_data_.append(f)\n",
    "\n",
    "        summary[exp]=filtered_data_[0]\n",
    "    # fig.savefig(f\"{opp}.png\")\n",
    "    x_labels = []\n",
    "    for col in summary.columns:\n",
    "        col = col.strip(\"output/\")\n",
    "        # col = col.strip(opp)\n",
    "        x_labels.append(col)\n",
    "    \n",
    "    ax.boxplot(summary)\n",
    "    ax.set_title(\"Lying rate\")\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.set_xticks(range(1,12), labels=x_labels)\n",
    "    ax.tick_params(axis='x', labelrotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# metrics = [\"defect_rate\", \"reciprocity_rate\", \"forgiveness_rate\", \"exploitability_rate\", \"retaliatory_rate\", \"troublemaking_rate\"]\n",
    "opponents = [\"-URND2p\", \"-RND2\", \"-URND2p03\"]\n",
    "\n",
    "summary = pd.DataFrame()\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 6))\n",
    "\n",
    "for opp in opponents:\n",
    "    for exp in glob.glob(f\"output/*baseline{opp}\"):\n",
    "        exp_res = os.path.join(exp, \"results.csv\")\n",
    "        df = pd.read_csv(exp_res)\n",
    "\n",
    "        df_metrics = pd.DataFrame()\n",
    "        metrics = [\"forgiveness_rate\"]\n",
    "        for m in metrics:\n",
    "            df_metrics[m] = df[m]\n",
    "\n",
    "        df_metrics.replace(-1, np.nan, inplace=True)\n",
    "        mask = ~np.isnan(df_metrics)\n",
    "        filtered_data = [df_metrics[d][mask[m]] for d, m in zip(df_metrics, mask)]\n",
    "        filtered_data_ = []\n",
    "        for f in filtered_data:\n",
    "            if not f.any():\n",
    "                f = [0]\n",
    "            filtered_data_.append(f)\n",
    "        \n",
    "        summary[exp]=filtered_data_[0]\n",
    "\n",
    "    exp_list = glob.glob(f\"output/*{opp}\")\n",
    "    exp_list.remove(exp)\n",
    "    exp_list.sort()\n",
    "\n",
    "    for i, exp in enumerate(exp_list):\n",
    "        exp_res = os.path.join(exp, \"results.csv\")\n",
    "        df = pd.read_csv(exp_res)\n",
    "\n",
    "        df_metrics = pd.DataFrame()\n",
    "        metrics = [\"forgiveness_rate\"]\n",
    "        for m in metrics:\n",
    "            df_metrics[m] = df[m]\n",
    "\n",
    "        df_metrics.replace(-1, np.nan, inplace=True)\n",
    "        mask = ~np.isnan(df_metrics)\n",
    "        filtered_data = [df_metrics[d][mask[m]] for d, m in zip(df_metrics, mask)]\n",
    "        filtered_data_ = []\n",
    "        for f in filtered_data:\n",
    "            if not f.any():\n",
    "                f = [0]\n",
    "            filtered_data_.append(f)\n",
    "\n",
    "        summary[exp]=filtered_data_[0]\n",
    "    # fig.savefig(f\"{opp}.png\")\n",
    "\n",
    "summary_joined = pd.DataFrame()\n",
    "\n",
    "ds = summary.filter(regex=r'(?i)baseline')\n",
    "baseline = []\n",
    "for d in ds:\n",
    "    baseline.extend(summary[d])\n",
    "summary_joined['baseline'] = baseline\n",
    "\n",
    "experiments = [\"agreeableness-minus\", \"agreeableness-plus\",\"conscientiousness-minus\", \"conscientiousness-plus\",\"extraversion-minus\", \"extraversion-plus\",\"neuroticism-minus\", \"neuroticism-plus\",\"openness-minus\", \"openness-plus\",]\n",
    "for exp in experiments:\n",
    "    ds = summary.filter(regex=r'(?i){}'.format(exp))\n",
    "    temp = []\n",
    "    for d in ds:\n",
    "        temp.extend(summary[d])\n",
    "    summary_joined[exp] = temp\n",
    "\n",
    "x_labels = []\n",
    "for col in summary_joined.columns:\n",
    "    # col = col.strip(\"output/\")\n",
    "    # col = col.strip(opp)\n",
    "    x_labels.append(col)\n",
    "    \n",
    "mask = ~np.isnan(summary_joined)\n",
    "filtered_data = [summary_joined[d][mask[m]] for d, m in zip(summary_joined, mask)]\n",
    "filtered_data_ = []\n",
    "for f in filtered_data:\n",
    "    if not f.any():\n",
    "        f = [0]\n",
    "    filtered_data_.append(f)\n",
    "    \n",
    "ax.boxplot(filtered_data_)\n",
    "ax.set_title(\"Forgiveness rate\")\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_xticks(range(1,12), labels=x_labels)\n",
    "ax.tick_params(axis='x', labelrotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# metrics = [\"defect_rate\", \"reciprocity_rate\", \"forgiveness_rate\", \"exploitability_rate\", \"retaliatory_rate\", \"troublemaking_rate\"]\n",
    "opponents = [\"*llmvllm*\"]\n",
    "\n",
    "for opp in opponents:\n",
    "    summary = pd.DataFrame()\n",
    "    exp_list = glob.glob(f\"output/{opp}\")\n",
    "    # exp_list.remove(exp)\n",
    "    exp_list.sort()\n",
    "\n",
    "    for i, exp in enumerate(exp_list):\n",
    "        exp_res = os.path.join(exp, \"results.csv\")\n",
    "        df = pd.read_csv(exp_res)\n",
    "\n",
    "        df_metrics = pd.DataFrame()\n",
    "        metrics = [\"score\",\"total_score\"]\n",
    "        for m in metrics:\n",
    "            df_metrics[m] = df[m]\n",
    "\n",
    "        summary[exp] = -1*df_metrics['total_score'] + 2*df_metrics['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\"baseline\", \"agreeableness-plus\",\"agreeableness-minus\", \"conscientiousness-plus\",\"conscientiousness-minus\",\"extraversion-plus\",\"extraversion-minus\",\"neuroticism-plus\",\"neuroticism-minus\",\"openness-plus\",\"openness-minus\"]\n",
    "\n",
    "summary_mean = summary.mean()\n",
    "summary_mean\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "scores = pd.DataFrame()\n",
    "\n",
    "for e1 in experiments:\n",
    "    scores[e1] = -1\n",
    "    for e2 in experiments:\n",
    "        scores.loc[e2] = -1\n",
    "\n",
    "for run in summary_mean.index:\n",
    "    run_name = run.split(\"_\")[0].split(\"/\")[-1]\n",
    "    splits = run_name.split(\"-\")\n",
    "    if splits[2] == \"baseline\":\n",
    "        p1 = \"baseline\"\n",
    "        if splits[3] == \"baseline\":\n",
    "            p2 = \"baseline\"\n",
    "        else: \n",
    "            p2 = \"-\".join(splits[3:5])\n",
    "    else:\n",
    "        p1 = \"-\".join(splits[2:4])\n",
    "        if splits[4] == \"baseline\":\n",
    "            p2 = \"baseline\"\n",
    "        else: \n",
    "            p2 = \"-\".join(splits[4:6])\n",
    "    scores.loc[p1,p2] = summary_mean[run]\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax = sns.heatmap(scores, annot=True)\n",
    "ax.set_title(\"Total score\")\n",
    "ax.set_ylabel(\"Prisoner A\")\n",
    "ax.set_xlabel(\"Prisoner B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax = sns.heatmap(scores, annot=True)\n",
    "ax.set_title(\"Personal score (A) difference\")\n",
    "ax.set_ylabel(\"Prisoner A\")\n",
    "ax.set_xlabel(\"Prisoner B\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repeng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
